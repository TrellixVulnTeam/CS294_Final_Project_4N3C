{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS 294 – CIFAR 10 – 2 class to memorization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS 294 – CIFAR 10 – Memorizing 100% of binary dataset\n"
      ],
      "metadata": {
        "id": "oUlT3V4TBmKc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ly-ZDHHRBjjS"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# Standard library imports\n",
        "import random\n",
        "\n",
        "# Third party imports\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import tqdm.notebook\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "RHVkMxVdHYar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the test and training data sets from CIFAR10, compressed to q20 in JPEG\n",
        "# and subsetted to only dog and cat classes for binary classification\n",
        "# files are available via Gihub\n",
        "\n",
        "!mkdir local_copy\n",
        "!cp /content/drive/MyDrive/gray_cat_dog_q_20_test.csv /content/local_copy\n",
        "!cp /content/drive/MyDrive/gray_cat_dog_q_20_train.csv /content/local_copy"
      ],
      "metadata": {
        "id": "vrY0Ao-dHd6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# classes for classification\n",
        "\n",
        "CIFAR10_CLASSES = np.array([\"cat\", \"dog\"])"
      ],
      "metadata": {
        "id": "VxGSqzKxC4A9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/local_copy/gray_cat_dog_q_20_train.csv\")\n",
        "df = np.array(df)\n",
        "\n",
        "test = np.array(pd.read_csv(\"/content/local_copy/gray_cat_dog_q_20_test.csv\"))"
      ],
      "metadata": {
        "id": "nXyOKtw4EALw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# perform a 50/50 split of 2-class test data, creating 5,000 images in the\n",
        "# training set and 5,000 images in the validation set\n",
        "# separate out labels from data set\n",
        "\n",
        "def split_CIFAR10(df, num_training=5_000, num_validation=5_000):\n",
        "  \"\"\"\n",
        "  Split CIFAR10 training data into training and validation data, with corresponding labels.\n",
        "  \"\"\"\n",
        "  X_train = df[:num_training, 0:-1].astype(np.float64)\n",
        "  _, y_train = np.unique(df[: num_training, -1], return_inverse=True)\n",
        "  print(X_train.shape)\n",
        "  print(y_train.shape)\n",
        "  X_val = df[num_training: num_training + num_validation, 0:-1].astype(np.float64)\n",
        "  _, y_val = np.unique(df[num_training: num_training + num_validation, -1], return_inverse=True)\n",
        "    \n",
        "  return {\n",
        "      \"X_train\": X_train, \"y_train\": y_train,\n",
        "      \"X_val\": X_val, \"y_val\": y_val\n",
        "  }\n",
        "\n",
        "CIFAR10_DATA = split_CIFAR10(df)\n",
        "\n",
        "# Create the test set as well, with no split (2,000 images)\n",
        "CIFAR10_TEST = split_CIFAR10(test, num_training=2_000)"
      ],
      "metadata": {
        "id": "7s3zDZ5tETp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale all of the input data by basing it on the training set and applying\n",
        "# the same scaling factor to valdiation and test set as well\n",
        "\n",
        "from sklearn import preprocessing\n",
        "scaler = preprocessing.StandardScaler().fit(CIFAR10_DATA['X_train'])\n",
        "X_scaled = scaler.transform(CIFAR10_DATA['X_train'])\n",
        "X_val_scaled = scaler.transform(CIFAR10_DATA['X_val'])\n",
        "X_test_scaled = scaler.transform(CIFAR10_TEST['X_train']) #it's just called that."
      ],
      "metadata": {
        "id": "VtOxnifRPzjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Build MLP models, scaling perceptrons from 1 to 13 (see papaer for rationale\n",
        "# behind the upper bound of 13; with 13 perceptrons, the network is expected to\n",
        "# generate a model at the expected MEC level of the data set)\n",
        "\n",
        "num_neurons = []\n",
        "train_acc = [] \n",
        "val_acc = []\n",
        "classifiers = []\n",
        "\n",
        "# From 1 to 13 neurons(expected MEC), generate the training and validation set\n",
        "# accuracy in order to plot the accuracy/capacity plot\n",
        "for i in range(1, 14):\n",
        "  print(\"iteration {}\".format(i))\n",
        "  num_neurons.append(i)\n",
        "  clf = MLPClassifier(hidden_layer_sizes=(i,), alpha=0, solver='lbfgs', random_state=1, max_iter=4000)\n",
        "  clf.fit(X_scaled, CIFAR10_DATA['y_train'])\n",
        "  classifiers.append(clf)\n",
        "  train_acc.append(clf.score(X_scaled, CIFAR10_DATA['y_train']))\n",
        "  val_acc.append(clf.score(X_val_scaled, CIFAR10_DATA['y_val']))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(num_neurons, train_acc, label='train_acc')\n",
        "ax.plot(num_neurons, val_acc, label='val_acc')\n",
        "print(train_acc, val_acc)\n",
        "ax.set_xlim(13,0)\n",
        "ax.set_ylim(0, 1)\n",
        "ax.set_title('Accuracy / Capacity curve for cat & dog')\n",
        "ax.set_xlabel('MEC')\n",
        "ax.set_ylabel('Accuracy')\n",
        "\n",
        "ax.grid(True)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "szVJ3xgGIQsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "num_neurons = []\n",
        "train_acc = [] \n",
        "val_acc = []\n",
        "classifiers = []\n",
        "\n",
        "# from 1 to 13 neurons(expected MEC)\n",
        "# generate the training and validation accuracy curves\n",
        "# using adam, max_iter = 4000\n",
        "# We ultimately chose this configuring for our paper\n",
        "\n",
        "for i in range(1, 14):\n",
        "  print(\"iteration {}\".format(i))\n",
        "  num_neurons.append(i)\n",
        "  clf = MLPClassifier(hidden_layer_sizes=(i,), alpha=0, solver='adam', random_state=1, max_iter=4000)\n",
        "  clf.fit(X_scaled, CIFAR10_DATA['y_train'])\n",
        "  classifiers.append(clf)\n",
        "  train_acc.append(clf.score(X_scaled, CIFAR10_DATA['y_train']))\n",
        "  val_acc.append(clf.score(X_val_scaled, CIFAR10_DATA['y_val']))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(num_neurons, train_acc, label='train_acc')\n",
        "ax.plot(num_neurons, val_acc, label='val_acc')\n",
        "print(train_acc, val_acc)\n",
        "ax.set_xlim(13,0)\n",
        "ax.set_ylim(0, 1)\n",
        "ax.set_title('Accuracy / Capacity curve for cat & dog')\n",
        "ax.set_xlabel('MEC')\n",
        "ax.set_ylabel('Accuracy')\n",
        "\n",
        "ax.grid(True)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cgdNp5FWxEGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "num_neurons = []\n",
        "train_acc = [] \n",
        "val_acc = []\n",
        "classifiers = []\n",
        "\n",
        "# from 1 to 13 neurons(expected MEC)\n",
        "# generate the training and validation accuracy curves\n",
        "# using lbfgs, max_iter scales with neurons\n",
        "\n",
        "for i in range(1, 14):\n",
        "  print(\"iteration {}\".format(i))\n",
        "  num_neurons.append(i)\n",
        "  max_iter = 4000 if i == 14 else 10/i*4000\n",
        "  clf = MLPClassifier(hidden_layer_sizes=(i,), alpha=0, solver='lbfgs', random_state=1, max_iter=max_iter)\n",
        "  clf.fit(X_scaled, CIFAR10_DATA['y_train'])\n",
        "  classifiers.append(clf)\n",
        "  train_acc.append(clf.score(X_scaled, CIFAR10_DATA['y_train']))\n",
        "  val_acc.append(clf.score(X_val_scaled, CIFAR10_DATA['y_val']))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(num_neurons, train_acc, label='train_acc')\n",
        "ax.plot(num_neurons, val_acc, label='val_acc')\n",
        "print(train_acc, val_acc)\n",
        "ax.set_xlim(13,0)\n",
        "ax.set_ylim(0, 1)\n",
        "ax.set_title('Accuracy / Capacity curve for cat & dog')\n",
        "ax.set_xlabel('MEC')\n",
        "ax.set_ylabel('Accuracy')\n",
        "\n",
        "ax.grid(True)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jLXKb7xFzF3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "num_neurons = []\n",
        "train_acc = [] \n",
        "val_acc = []\n",
        "classifiers = []\n",
        "\n",
        "# from 1 to 13 neurons(expected MEC)\n",
        "# generate the training and validation accuracy curves\n",
        "# using adam, max_iter = 4000\n",
        "\n",
        "for i in range(1, 14):\n",
        "  print(\"iteration {}\".format(i))\n",
        "  num_neurons.append(i)\n",
        "  clf = MLPClassifier(hidden_layer_sizes=(i,2), alpha=0, solver='adam', random_state=1, max_iter=4000)\n",
        "  clf.fit(X_scaled, CIFAR10_DATA['y_train'])\n",
        "  classifiers.append(clf)\n",
        "  train_acc.append(clf.score(X_scaled, CIFAR10_DATA['y_train']))\n",
        "  val_acc.append(clf.score(X_val_scaled, CIFAR10_DATA['y_val']))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(num_neurons, train_acc, label='train_acc')\n",
        "ax.plot(num_neurons, val_acc, label='val_acc')\n",
        "print(train_acc, val_acc)\n",
        "ax.set_xlim(13,0)\n",
        "ax.set_ylim(0, 1)\n",
        "ax.set_title('Accuracy / Capacity curve for cat & dog')\n",
        "ax.set_xlabel('MEC')\n",
        "ax.set_ylabel('Accuracy')\n",
        "\n",
        "ax.grid(True)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kzWrpu7H2ZF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combine training and validation data sets together so that we can\n",
        "# train on the entire training set \n",
        "\n",
        "x_full_train_dataset = []\n",
        "\n",
        "for num, data in enumerate(X_scaled):\n",
        "  x_full_train_dataset.append(data)\n",
        "for num, data in enumerate(X_val_scaled):\n",
        "  x_full_train_dataset.append(data)\n",
        "\n",
        "y_full_train_dataset = []\n",
        "\n",
        "for num, data in enumerate(X_scaled):\n",
        "  y_full_train_dataset.append(CIFAR10_DATA['y_train'][num])\n",
        "for num, data in enumerate(X_val_scaled):\n",
        "  y_full_train_dataset.append(CIFAR10_DATA['y_val'][num])\n",
        "\n"
      ],
      "metadata": {
        "id": "GFEqnvKwfmXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# i - 7 since we chose to use 7 neurons\n",
        "i = 7\n",
        "\n",
        "# so now let's fit the model on the entire data set\n",
        "clf = MLPClassifier(hidden_layer_sizes=(i,), alpha=0, solver='adam', random_state=1, max_iter=4000)\n",
        "clf.fit(x_full_train_dataset, y_full_train_dataset)\n",
        "classifiers.append(clf)\n",
        "\n",
        "# output training and test accuracy\n",
        "print(clf.score(x_full_train_dataset, y_full_train_dataset))\n",
        "print(clf.score(X_test_scaled, CIFAR10_TEST['y_train']))"
      ],
      "metadata": {
        "id": "RV1ivOPBiO8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the dataset in case\n",
        "\n",
        "np.savetxt(\"/content/local_copy/scaled_combined_train_q20_cat_dog.csv\",combined_dataset,delimiter=\",\",)\n"
      ],
      "metadata": {
        "id": "G4Yw0va-dxZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZqX73mCceI5H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}